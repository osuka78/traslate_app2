import streamlit as st
import google.generativeai as genai
from google.api_core import exceptions

# --- UI Enhancement with Custom CSS ---
def apply_premium_styles():
    st.markdown("""
        <style>
        @import url('https://fonts.googleapis.com/css2?family=Outfit:wght@300;400;600;800&display=swap');
        
        .stApp {
            background: linear-gradient(135deg, #f8faff 0%, #e8efff 100%);
            font-family: 'Outfit', sans-serif;
        }

        .header-box {
            background: rgba(255, 255, 255, 0.7);
            backdrop-filter: blur(12px);
            border-radius: 20px;
            padding: 20px;
            border: 1px solid rgba(255, 255, 255, 0.4);
            box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.05);
            margin-bottom: 25px;
            text-align: center;
        }

        .gradient-text {
            background: linear-gradient(90deg, #2563eb, #7c3aed);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-weight: 800;
            font-size: 2.2rem;
            margin: 0;
        }

        /* ã‚«ãƒ©ãƒ ã®ä¸­ã®ã‚«ãƒ¼ãƒ‰é¢¨ã‚¹ã‚¿ã‚¤ãƒ« */
        .column-card {
            background: white;
            border-radius: 20px;
            padding: 25px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.03);
            border: 1px solid #f0f2f6;
            height: 100%;
        }

        [data-testid="stStatusWidget"] {
            display: none;
        }
        
        .stForm {
            border: none !important;
            padding: 0 !important;
        }
        </style>
    """, unsafe_allow_html=True)

# Page Config (Wide Layout for side-by-side)
st.set_page_config(page_title="Smart Business Comm", page_icon="ğŸ’¬", layout="wide")
apply_premium_styles()

# --- Gemini Configuration ---
API_KEY = st.secrets["GOOGLE_API_KEY"]
genai.configure(api_key=API_KEY)

# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®å„ªå…ˆé †ä½ãƒªã‚¹ãƒˆï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ã®ãƒªã‚¹ãƒˆã«åŸºã¥ãï¼‰
MODEL_PRIORITY = [
    'models/gemini-2.5-flash', 
    'models/gemini-2.0-flash', 
    'models/gemini-2.5-flash-lite', 
    'models/gemini-2.5-pro', 
    'models/gemini-pro-latest',
    'models/gemini-exp-1206'
]

def generate_with_fallback(prompt):
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãŒç™ºç”Ÿã—ãŸå ´åˆã«ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã¦å†è©¦è¡Œã™ã‚‹é–¢æ•°"""
    last_exception = None
    for model_name in MODEL_PRIORITY:
        try:
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            return response, model_name
        except exceptions.ResourceExhausted:
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æ¬¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™
            last_exception = f"Rate limit reached for {model_name}"
            continue
        except Exception as e:
            # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã¯ãã®ã¾ã¾ã‚¹ãƒ­ãƒ¼
            raise e
    
    if last_exception:
        raise Exception(f"ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸ: {last_exception}")
    raise Exception("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

# Header Area
st.markdown('<div class="header-box"><h1 class="gradient-text">ğŸ’¬ Mail & Chat Assistant</h1><p style="color: #64748b; margin-top:5px;">ç›¸æ‰‹ã®ãƒˆãƒ¼ãƒ³ã‚’èª­ã¿å–ã‚Šã€æœ€é©ãªè¿”ä¿¡ã‚’å·¦å³ã§åŒæ™‚ã‚µãƒãƒ¼ãƒˆ</p></div>', unsafe_allow_html=True)

# Session State
if 'last_incoming' not in st.session_state:
    st.session_state.last_incoming = ""

# --- Create Two Columns ---
col1, col2 = st.columns([1, 1], gap="large")

# ==========================================
# Left Column: Incoming Translation
# ==========================================
with col1:
    st.markdown("### ğŸ“¥ å±Šã„ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ (è‹± â†’ æ—¥)")
    st.caption("è‹±èªã‚’è²¼ã‚Šä»˜ã‘ã‚‹ã¨è‡ªå‹•ã§ç¿»è¨³ãƒ»ãƒˆãƒ¼ãƒ³åˆ†æã‚’è¡Œã„ã¾ã™")
    
    @st.fragment
    def translation_fragment():
        incoming_text = st.text_area(
            "Receive Area", 
            height=200, 
            placeholder="ã“ã“ã«ç›¸æ‰‹ã‹ã‚‰ã®ãƒ¡ãƒ¼ãƒ«ã‚„ãƒãƒ£ãƒƒãƒˆã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„",
            key="inc_input_area_wide",
            label_visibility="collapsed"
        )

        if incoming_text:
            st.session_state.last_incoming = incoming_text
            status_msg = st.empty()
            status_msg.caption("âŒ› åˆ†æä¸­...")
            try:
                prompt = f"""
                ä»¥ä¸‹ã®è‹±èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚
                [æŒ‡ç¤º]:
                1. åª’ä½“ï¼ˆãƒ¡ãƒ¼ãƒ«/ãƒãƒ£ãƒƒãƒˆï¼‰ã¨ç›¸æ‰‹ã®ãƒˆãƒ¼ãƒ³ã‚’åˆ†æã—ã€æœ€é©ãªæ—¥æœ¬èªã§ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚
                [è‹±èªãƒ†ã‚­ã‚¹ãƒˆ]: {incoming_text}
                """
                response, used_model = generate_with_fallback(prompt)
                status_msg.empty()
                st.markdown(f"#### ğŸ‡¯ğŸ‡µ ç¿»è¨³ã¨åˆ†æçµæœ (`{used_model}`)")
                st.info(response.text)
            except Exception as e:
                status_msg.empty()
                st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            st.info("å·¦å´ã®ãƒœãƒƒã‚¯ã‚¹ã«ç¿»è¨³ã—ãŸã„æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    translation_fragment()


# ==========================================
# Right Column: Reply Creation
# ==========================================
with col2:
    st.markdown("### ğŸ“¤ è¿”ä¿¡ã®ä½œæˆ (æ—¥ â†’ è‹±)")
    st.caption("å·¦å´ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚‹å ´åˆã€ãã®ãƒˆãƒ¼ãƒ³ã‚’è€ƒæ…®ã—ã¾ã™")
    
    @st.fragment
    def reply_fragment():
        with st.form("reply_form_wide"):
            reply_text = st.text_area(
                "Reply Area", 
                height=200, 
                placeholder="ä¾‹ï¼šäº†è§£ã—ã¾ã—ãŸã€‚æ˜æ—¥ã¾ã§ã«ç¢ºèªã—ã¦é€£çµ¡ã—ã¾ã™ã€‚",
                key="reply_input_area_wide",
                label_visibility="collapsed"
            )
            submit_button = st.form_submit_button("âœ¨ è‹±èªã®è¿”ä¿¡æ¡ˆã‚’ç”Ÿæˆ")

            if submit_button:
                if reply_text.strip():
                    status_msg_reply = st.empty()
                    status_msg_reply.caption("âŒ› ç›¸æ‰‹ã«åˆã‚ã›ãŸæ¡ˆã‚’æ§‹æˆä¸­...")
                    try:
                        ref_text = f"ç›¸æ‰‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {st.session_state.last_incoming}" if st.session_state.last_incoming else "ãªã—"
                        prompt = f"""
                        ãƒ—ãƒ­ã®ãƒ“ã‚¸ãƒã‚¹ç¿»è¨³è€…ã¨ã—ã¦ã€æœ€é©ãªè‹±èªè¿”ä¿¡æ¡ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
                        [ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ]: {ref_text}
                        [å…¥åŠ›æ—¥æœ¬èª]: {reply_text}
                        [å‡ºåŠ›æ§‹æˆ]:
                        1. AIã‚ªã‚¹ã‚¹ãƒ¡ï¼ˆç›¸æ‰‹ã®ãƒˆãƒ¼ãƒ³ã¨åŒæœŸï¼‰
                           - è‹±æ–‡
                           - æ¡ç”¨ç†ç”±ï¼ˆâ€»å¿…ãšæ—¥æœ¬èªã§èª¬æ˜ã—ã¦ãã ã•ã„ï¼‰
                        2. Formalï¼ˆä¸å¯§ï¼‰
                           - è‹±æ–‡ã¨æ—¥æœ¬èªè¨³
                        3. Casualï¼ˆç°¡æ½”ï¼‰
                           - è‹±æ–‡ã¨æ—¥æœ¬èªè¨³

                        [é‡è¦ãªæŒ‡ç¤º]:
                        - è‹±æ–‡ãŒé©åˆ‡ã§ã‚ã‚‹ç†ç”±ã‚„ã€ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ã®è§£èª¬ã¯ã€ã™ã¹ã¦**æ—¥æœ¬èª**ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
                        """
                        response, used_model = generate_with_fallback(prompt)
                        status_msg_reply.empty()
                        st.markdown("---")
                        st.markdown(f"#### ğŸ“ AIã‹ã‚‰ã®ææ¡ˆ (`{used_model}`)")
                        st.markdown(response.text)
                    except Exception as e:
                        status_msg_reply.empty()
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
                else:
                    st.warning("è¿”ä¿¡ã—ãŸã„å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    reply_fragment()

st.markdown("""
<br><br>
<div style="text-align: center; color: #94a3b8; font-size: 0.8rem;">
    Side-by-Side Context Sync â€¢ Multi-Model Fallback Support
</div>
""", unsafe_allow_html=True)
